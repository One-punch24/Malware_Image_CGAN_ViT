# coding=utf-8
from __future__ import absolute_import, division, print_function

import logging
import argparse
import os
import random
import numpy as np
import time 
import sys
from tqdm import tqdm

from datetime import timedelta
import torch
from torch.autograd import Variable
import torch.nn as nn

from ViT.ViTransformer import VisionTransformer, CONFIGS
from utils.dataset_utils import get_loader
#from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule

logger = logging.getLogger(__name__)

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def simple_accuracy(preds, labels):
    return (preds == labels).mean()


def save_checkpoint(state,filename):
    torch .save(state,filename)

def count_parameters(model):
    params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return params/1000000

def setup(args):
    # Prepare model
    config = CONFIGS[args.model_type]
    num_classes = 26
    model = VisionTransformer(config, args.img_size, num_classes=num_classes,zero_init_classifierhead=True)
    if args.start_epoch==0:
        print("  Loading from pretrained model...")
        model.load_from(np.load(args.pretrained_dir))
    else:
        print("  Loading from model of epoch_",args.start_epoch,'...')
        model.load_state_dict(torch.load(args.ckpt_dir+'model_epoch_'+str(args.start_epoch)+'.ckpt'))
    if torch.cuda.is_available():
        model.cuda()
    num_params = count_parameters(model)

    logger.info("{}".format(config))
    logger.info("Training parameters %s", args)
    logger.info("Total Parameter: \t%2.1fM" % num_params)
    print("  The amount of parameters: ", num_params)
    return args, model

def main():
    parser = argparse.ArgumentParser()
    # Required parameters
    parser.add_argument("--test", default=False,
                        help="Mode of testing or training")
    parser.add_argument("--model_type", choices=["ViT-B_16"],
                        default="ViT-B_16", help="Which variant to use.")
    parser.add_argument("--pretrained_dir", type=str, default="D:\REPO\checkpoint\ViT-B_16.npz",
                        help="Where to search for pretrained ViT models.")
    parser.add_argument("--ckpt_dir", type=str, default="saved_ckpt\\",
                        help="Where to search for pretrained ViT models.")
    parser.add_argument("--output_dir", default="output", type=str,
                        help="The output directory where checkpoints will be written.")
    parser.add_argument("--img_size", default=224, type=int,
                        help="Resolution size")
    parser.add_argument("--train_batch_size", default=8, type=int,
                        help="Total batch size for training.")
    parser.add_argument("--eval_batch_size", default=8, type=int,
                        help="Total batch size for eval.")
    parser.add_argument("--eval_every", default=100, type=int,
                        help="Run prediction on validation set every so many steps."
                             "Will always run one evaluation at the end of training.")
    parser.add_argument("--learning_rate", default=3e-2, type=float,
                        help="The initial learning rate for SGD.")
    parser.add_argument("--weight_decay", default=0, type=float,
                        help="Weight deay if we apply some.")
    parser.add_argument("--epochs", default=3, type=int,
                        help="Total number of training epochs to perform.")
    parser.add_argument("--start_epoch", default=0, type=int,
                         help="Load from model_epoch_i.ckpt")
    parser.add_argument("--dataset_dir", default="D:\REPO\dataset\malevis_train_val_224x224", type=str,
                         help="Load from model_epoch_i.ckpt")

    # parser.add_argument("--decay_type", choices=["cosine", "linear"], default="cosine",
    #                     help="How to decay the learning rate.")
    # parser.add_argument("--warmup_steps", default=500, type=int,
    #                     help="Step of training to perform learning rate warmup for.")
    parser.add_argument("--max_grad_norm", default=1.0, type=float,
                        help="Max gradient norm.")
    parser.add_argument('--gradient_accumulation_steps', type=int, default=8,
                        help="Number of updates steps to accumulate before performing a backward/update pass.")
    args = parser.parse_args()
    args.print_freq=20
    # Model & Tokenizer Setup
    args, model = setup(args)
    train_loader,test_loader=get_loader(args)
    criterion=nn.CrossEntropyLoss()
    if torch.cuda.is_available():
        criterion.cuda()
    best_prec1 = 0
    start_epoch= 0
    optimizer = torch.optim.SGD(model.parameters(),
                                lr=args.learning_rate,
                                momentum=0.9,
                                weight_decay=args.weight_decay)
    '''
    Test, Validation:
    '''
    if args.test:
        validate(test_loader,model,criterion,args)
        return 0
    '''
    Train:
    '''
    print("  Training......")
    for epoch in range(start_epoch,args.epochs):
        train(train_loader,model,criterion,optimizer,epoch,args)
        prec1=validate(test_loader,model,criterion,args)
        best_prec1=max(prec1,best_prec1)
        print("  Best validation result: ", best_prec1)
        save_checkpoint(model.state_dict(),'saved_ckpt\\model_epoch_'+str(epoch)+'.ckpt')

def accuracy(output,target,topk=(1,)):
    with torch.no_grad():
        maxk=max(topk)
        batch_size=target.size(0)
        _,pred=output.topk(maxk,1,True,True)
        pred =pred.t()
        correct=pred.eq(target.view(1,-1).expand_as(pred))
        res=[]
        for k in topk:
            correct_k=correct[:k].view(-1).float().sum(0,keepdim=True)
            res.append(correct_k.mul_(100.0/batch_size))
        return res
'''
Step = Batch idx;
'''
def validate(val_loader, model, criterion,args):
    """
    Run evaluation
    """
    print("  Validating......")
    losses = AverageMeter()
    top1 = AverageMeter()
    batch_time=AverageMeter()
    model.eval()

    end = time.time()
    for i, data in enumerate(tqdm(val_loader,bar_format="{l_bar}{r_bar}",
    miniters=args.gradient_accumulation_steps*args.print_freq)):
      
        input = Variable(data[0].cuda(non_blocking=True))
        target =Variable(data[1].cuda(non_blocking=True))
        with torch.no_grad():
            output,_ = model(input)
            loss = criterion(output, target)
        output = output.float()
        loss = loss.float()

        prec1 = accuracy(output.data, target)[0]
        losses.update(loss.item())
        top1.update(prec1.item())
        batch_time.update(time.time()-end)
        end=time.time()
        if (i+1)%(args.gradient_accumulation_steps*args.print_freq)==0:
            # with redirect_to_tqdm():
            #     time.sleep(.1)

            print(' \n' 

                      '  Test: [{0}/{1}]\t'
                      '  Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      '  Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      '  Prec@1 {top1.val:.3f} ({top1.avg:.3f})'
                      .format(i,len(val_loader),
                      loss=losses,batch_time=batch_time,top1=top1))


    return top1.avg

def train(train_loader,model,criterion,optimizer,epoch,args):
    losses=AverageMeter()
    top1=AverageMeter()
    batch_time=AverageMeter()

    model.train()
    end=time.time()
    for i, (data,target) in enumerate(tqdm(train_loader,bar_format="{l_bar}{r_bar}",
    miniters=args.gradient_accumulation_steps*args.print_freq)):
        data =data.cuda(non_blocking=True)
        target=target.cuda(non_blocking=True)

        output,_=model(data)
        # loss =criterion(output,target)
        # Imitate the accumulate gradient
        loss =criterion(output,target)/args.gradient_accumulation_steps
        loss.backward()

        # Ensure a float tensor
        output=output.float()
        loss=loss.float()
        prec1=accuracy(output.data,target)[0]
        #loss.item()*args.gradient_accumulation_steps
        losses.update(loss.item()*args.gradient_accumulation_steps)
        top1.update(prec1.item())

        batch_time.update(time.time()-end)
        end=time.time()
        '''
        Accumulate the gradient:
        '''
        if (i+1)% (args.gradient_accumulation_steps)==0:
            optimizer.step()
            optimizer.zero_grad()
            # Control print frequency based on the number of batches
            batch_idx=(i+1)/(args.gradient_accumulation_steps)

            if (i+1)%(args.gradient_accumulation_steps*args.print_freq)==0:
                
                print(' \n' 
                      '  Test: [{0}/{1}]\t '
                      '  Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      '  Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      '  Prec@1 {top1.val:.3f} ({top1.avg:.3f})'
                      .format(i,len(train_loader),batch_time=batch_time,
                      loss=losses,top1=top1))
if __name__=='__main__':
    main()






    

'''   
if __name__=='__main__':
     main()

    # parser.add_argument('--fp16', action='store_true',
    #                     help="Whether to use 16-bit float precision instead of 32-bit")
    # parser.add_argument('--fp16_opt_level', type=str, default='O2',
    #                     help="For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
    #                          "See details at https://nvidia.github.io/apex/amp.html")
    # parser.add_argument('--loss_scale', type=float, default=0,
    #                     help="Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n"
    #                          "0 (default value): dynamic loss scaling.\n"
    #                          "Positive power of 2: static loss scaling value.\n")
'''