# coding=utf-8
#88.06%
from __future__ import absolute_import, division, print_function

import logging
import argparse
import os
import random
import numpy as np
import time 
import sys
from tqdm import tqdm

from datetime import timedelta
import torch
from torch.autograd import Variable
import torch.nn as nn

from ViT.ViTransformer import VisionTransformer, CONFIGS
from utils.dataset_utils import get_loader
#from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger = logging.getLogger(__name__)

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def simple_accuracy(preds, labels):
    return (preds == labels).mean()


def save_checkpoint(state,filename):
    torch.save(state,filename)

def count_parameters(model):
    params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return params/1000000

def setup(args):
    # Prepare model
    config = CONFIGS[args.model_type]
    num_classes = 2
    model = VisionTransformer(config, args.img_size, num_classes=num_classes,zero_init_classifierhead=True)
    if args.start_epoch==0:
        print("  Loading from pretrained model...")
        model.load_from(np.load(args.pretrained_dir))
    else:
        print("  Loading from model of epoch_",args.start_epoch,'...')
        model.load_state_dict(torch.load(args.ckpt_dir+'model_epoch_'+str(args.start_epoch)+'.ckpt'))
    if torch.cuda.is_available():
        model.cuda()
    num_params = count_parameters(model)

    logger.info("{}".format(config))
    logger.info("Training parameters %s", args)
    logger.info("Total Parameter: \t%2.1fM" % num_params)
    print("  The amount of parameters: ", num_params)
    return args, model

def main():
    parser = argparse.ArgumentParser()
    # Required parameters
    parser.add_argument("--test", default=False,
                        help="Mode of testing or training")
    parser.add_argument("--model_type", choices=["ViT-B_16"],
                        default="ViT-B_16", help="Which variant to use.")
    parser.add_argument("--pretrained_dir", type=str, default="ViT-B_16.npz",
                        help="Where to search for pretrained ViT models.")
    parser.add_argument("--ckpt_dir", type=str, default="saved_ckpt\\",
                        help="Where to search for pretrained ViT models.")
    parser.add_argument("--output_dir", default="output", type=str,
                        help="The output directory where checkpoints will be written.")
    parser.add_argument("--img_size", default=224, type=int,
                        help="Resolution size")
    parser.add_argument("--train_batch_size", default=8, type=int,
                        help="Total batch size for training.")
    parser.add_argument("--eval_batch_size", default=8, type=int,
                        help="Total batch size for eval.")
    parser.add_argument("--eval_every", default=100, type=int,
                        help="Run prediction on validation set every so many steps."
                             "Will always run one evaluation at the end of training.")
    parser.add_argument("--learning_rate", default=9e-2, type=float,
                        help="The initial learning rate for SGD.")
    parser.add_argument("--weight_decay", default=0, type=float,
                        help="Weight deay if we apply some.")
    parser.add_argument("--epochs", default=30, type=int,
                        help="Total number of training epochs to perform.")
    parser.add_argument("--start_epoch", default=0, type=int,
                         help="Load from model_epoch_i.ckpt")
    parser.add_argument("--dataset_dir", default="samples\\", type=str,
                         help="Load from model_epoch_i.ckpt")

    # parser.add_argument("--decay_type", choices=["cosine", "linear"], default="cosine",
    #                     help="How to decay the learning rate.")
    # parser.add_argument("--warmup_steps", default=500, type=int,
    #                     help="Step of training to perform learning rate warmup for.")
    parser.add_argument("--max_grad_norm", default=1.0, type=float,
                        help="Max gradient norm.")
    parser.add_argument('--gradient_accumulation_steps', type=int, default=8,
                        help="Number of updates steps to accumulate before performing a backward/update pass.")
    args = parser.parse_args()
    args.print_freq=20
    # Model & Tokenizer Setup
    args, model = setup(args)
    train_loader,test_loader=get_loader(args)
    criterion=nn.CrossEntropyLoss()
    if torch.cuda.is_available():
        criterion.cuda()
    best_prec1 = 0
    start_epoch= 0
    optimizer = torch.optim.SGD(model.parameters(),
                                lr=args.learning_rate,
                                momentum=0.9,
                                weight_decay=args.weight_decay)
    CosineLR = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 
                                T_max=10, eta_min=3e-4)
    '''
    Test, Validation:
    '''
    if args.test:
        validate(test_loader,model,criterion,args)
        return 0
    '''
    Train:
    '''
    print("  Training......, "+str(args.epochs)+"epochs")
    for epoch in range(start_epoch,args.epochs):
        print("************* Epoch "+str(epoch))
        train(train_loader,model,criterion,optimizer,epoch,args)
        prec1, tp, fp, tn, fn, largePrec=validate(test_loader,model,criterion,args)
        best_prec1=max(prec1,best_prec1)
        print("  Best validation result: ", best_prec1)
        print("true positive: ", tp)
        print("false positive: ", fp)
        print("true negative: ", tn)
        print("false negative: ", fn)
        print("precision for larger: ",largePrec)
        save_checkpoint(model.state_dict(),args.ckpt_dir+'model_epoch_'+str(epoch)+'.ckpt')
        CosineLR.step()

def accuracy(output,target, input_=None, topk=(1,)):
    with torch.no_grad():
        maxk=max(topk)
        batch_size=target.size(0)
        _,pred=output.topk(maxk,1,True,True)
        pred =pred.t()
        isLarge = True
        if input_ is not None:
            for ind in input_:
                if 1-np.sum(ind.numpy()==-1)/150528 < (3*500) / (3*224*224):
                    isLarge = False
                    break
        correct=pred.eq(target.view(1,-1).expand_as(pred))
        res=[]
        for k in topk:
            correct_k=correct[:k].view(-1).float().sum(0,keepdim=True)
            res.append(correct_k.mul_(100.0/batch_size))
        return res, isLarge

def TPFPTNFN(output,target):
    with torch.no_grad():
        fp = tp = tn = fn = 0
        _,pred=output.topk(1,1,True,True)
        pred =pred.t()
        output_CPU = pred.cpu().numpy()[0]
        target_CPU = target.cpu().numpy()
        for i in range(len(output_CPU)):
            if output_CPU[i] == 1 and target_CPU[i] != 1:
                fp += 1
            if output_CPU[i] == 1 and target_CPU[i] == 1:
                tp += 1
            if output_CPU[i] != 1 and target_CPU[i] == 1:
                fn += 1
            if output_CPU[i] != 1 and target_CPU[i] != 1:
                tn += 1

        return tp, fp, tn, fn


'''
Step = Batch idx;
'''
def validate(val_loader, model, criterion,args):
    """
    Run evaluation
    """
    print("  Validating......")
    losses = AverageMeter()
    top1 = AverageMeter()
    batch_time=AverageMeter()
    largePrec = AverageMeter()

    TruePos = AverageMeter()
    FalsePos = AverageMeter()
    TrueNeg = AverageMeter()
    FalseNeg = AverageMeter()
    model.eval()

    end = time.time()
    for i, data in enumerate(tqdm(val_loader,bar_format="{l_bar}{r_bar}",
    miniters=args.gradient_accumulation_steps*args.print_freq)):
        input_CPU = Variable(data[0])
        input = Variable(data[0].cuda(non_blocking=True))
        target =Variable(data[1].cuda(non_blocking=True))
        with torch.no_grad():
            output,_ = model(input)
            loss = criterion(output, target)
        output = output.float()
        loss = loss.float()

        prec1, isLarge = accuracy(output.data, target, input_CPU)
        prec1 = prec1[0]
        tp, fp, tn, fn = TPFPTNFN(output.data, target)
        TruePos.update(tp)
        FalsePos.update(fp)
        TrueNeg.update(tn)
        FalseNeg.update(fn)

        losses.update(loss.item())
        top1.update(prec1.item())
        if isLarge:
            largePrec.update(prec1.item())
        batch_time.update(time.time()-end)
        end=time.time()
        if (i+1)%(args.gradient_accumulation_steps*args.print_freq)==0:
            # with redirect_to_tqdm():
            #     time.sleep(.1)

            print(' \n' 

                      '  Test: [{0}/{1}]\t'
                      '  Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      '  Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      '  Prec@1 {top1.val:.3f} ({top1.avg:.3f})\n'
                      '  Large Prec@1 {largePrec.val:.3f} ({largePrec.avg:.3f})\t'
                      .format(i,len(val_loader),
                      loss=losses,batch_time=batch_time,top1=top1, largePrec=largePrec))

    return top1.avg, TruePos.sum/(TruePos.sum+FalsePos.sum), FalsePos.sum/(TruePos.sum+FalsePos.sum), TrueNeg.sum/(TrueNeg.sum+FalseNeg.sum), FalseNeg.sum/(TrueNeg.sum+FalseNeg.sum), largePrec.avg

def train(train_loader,model,criterion,optimizer,epoch,args):
    losses=AverageMeter()
    top1=AverageMeter()
    batch_time=AverageMeter()

    model.train()
    end=time.time()
    for i, (data,target) in enumerate(tqdm(train_loader,bar_format="{l_bar}{r_bar}",
    miniters=args.gradient_accumulation_steps*args.print_freq)):
        data_CPU = data
        data =data.cuda(non_blocking=True)
        target=target.cuda(non_blocking=True)
        output,_=model(data)
        # Imitate the accumulate gradient
        loss =criterion(output,target)/args.gradient_accumulation_steps
        loss.backward()

        # Ensure a float tensor
        output=output.float()
        loss=loss.float()
        prec1, nothing=accuracy(output.data,target)
        prec1 = prec1[0]
        #loss.item()*args.gradient_accumulation_steps
        losses.update(loss.item()*args.gradient_accumulation_steps)
        top1.update(prec1.item())

        batch_time.update(time.time()-end)
        end=time.time()
        '''
        Accumulate the gradient:
        '''
        if (i+1)% (args.gradient_accumulation_steps)==0:
            optimizer.step()
            optimizer.zero_grad()
            # Control print frequency based on the number of batches
            batch_idx=(i+1)/(args.gradient_accumulation_steps)

            if (i+1)%(args.gradient_accumulation_steps*args.print_freq)==0:
                
                print(' \n' 
                      '  Test: [{0}/{1}]\t '
                      '  Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      '  Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      '  Prec@1 {top1.val:.3f} ({top1.avg:.3f})'
                      .format(i,len(train_loader),batch_time=batch_time,
                      loss=losses,top1=top1))
if __name__=='__main__':
    main()
