from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import copy
import logging
import math

import torch
import torch.nn as nn
import numpy as np

from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm
from torch.nn.modules.utils import _pair
from scipy import ndimage

model=np.load("D:\REPO\checkpoint\ViT-B_16.npz")
print(model)
lst = model.files
for item in lst:
    print(item)
    #print(model[item])

class Attention(nn.Module):
    def __init__(self,config,vis):
        super(Attention,self).__init__()
        self.vis=vis
        self.num_attention_heads=config.transformer["num_heads"]
        # hidden vector is concated by num_head heads output
        self.attention_head_size=int(config.hidden_size/self.num_attention_heads)
        self.all_head_size=self.num_attention_heads*self.attention_head_size

        # Linear transformation to obtain Q,K,V
        self.query = Linear(config.hidden_size, self.all_head_size)
        self.key = Linear(config.hidden_size, self.all_head_size)
        self.value = Linear(config.hidden_size, self.all_head_size)

        # Linear projection for weighted sum
        self.out = Linear(config.hidden_size, config.hidden_size)

        # Dropout 
        self.attn_dropout=Dropout(config.transformer["attention_dropout_rate"])
        self.proj_dropout=Dropout(config.transformer["attention_dropout_rate"])

        # dim=-1 ~ last dim
        self.softmax = Softmax(dim=-1)

    def transpose_for_scores(self,x):
        # (Batchsize * 256* hidden_channels)->(Batchsize * 256* num_heads * attention_head_size)
        new_x_shape=x.size()[:-1] +(self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        
        return x.permute(0, 2, 1, 3)


    def forward(self,hidden_states):
        # (Batchsize * 256* hidden_channels)-> (Batchsize * 256*all_head_size)
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)

        # (Batchsize , num_heads ,256, attention_head_size)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        '''
        Attention weighted sum:
        '''

        # torch.mutual: do multiple in end 2 dimentions forever.
        # broadcast: non-singleton dimension must match exactly.
        # attention_scores: (Batchsize , num_heads ,256_query,256)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        # attention_probs: (Batchsize , num_heads ,256_query,256)
        # for every image position piece, it will get a attention map for other pieces;
        attention_probs = self.softmax(attention_scores)
        weights = attention_probs if self.vis else None
        attention_probs = self.attn_dropout(attention_probs)
        '''
        Context layer:
        '''
        #context_layer: (Batchsize , num_heads ,256_query,attention_head_size) --> weighted sum
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        #(Batchsize ,256_query, num_heads ,attention_head_size)
        new_context_layer_shape = context_layer.size()[:-2] +\
         (self.all_head_size,)
        # (Batchsize ,256_query, attention_head_size,num_heads )
        # -->(Batchsize ,256_query, hidden_size )
        context_layer = context_layer.view(*new_context_layer_shape)

        '''
        Attention final linear projection map:
        '''
        attention_output = self.out(context_layer)
        attention_output = self.proj_dropout(attention_output)
        return attention_output, weights

# Pay attention that our input embedding 
# shape is B * Hidden_size * Length
class Embeddings(nn.Module):
    def __init__(self,config,img_size,in_channels=3):
        self.hybrid=None
        # generate a tuple   3-->(3,3)
        img_size=_pair(img_size) 

        patch_size=_pair(config.patches["size"])
        n_patches=(img_size[0]//patch_size[0])*(img_size[1] // patch_size[1])
        # Convolution filter for image
        # Output: 16*16*hidden_size
        # (N,C_in,H,W) -> (N,C_out,H,W)
        self.patch_embeddings = Conv2d(in_channels=in_channels,
                                       out_channels=config.hidden_size,
                                       kernel_size=patch_size,
                                       stride=patch_size)
        # Turn tensor to trainable var
        self.position_embeddings=nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))
        # config.hidden_size=768=
        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))

        self.dropout = Dropout(config.transformer["dropout_rate"])

    def forward(self, x):
        Batch_size = x.shape[0]
        # CLS_tokens: (Batch_size,1,hidden_channels)
        cls_tokens = self.cls_token.expand(Batch_size, -1, -1)     

        x=self.patch_embeddings(x)
        # Before Flatten: (Batchsize * hidden_channels * 16 * 16)
        # Flatten from dim 2: (Batchsize * hidden_channels * 256)
        x=x.flatten(2)
        # x: (Batchsize * 256* hidden_channels)
        x=x.transpose(-1,-2)
        # x: (Batchsize * 256+1 * hidden_channels)
        x = torch.cat((cls_tokens, x), dim=1)
        embeddings = x + self.position_embeddings
        embeddings = self.dropout(embeddings)
        return embeddings
